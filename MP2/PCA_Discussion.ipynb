{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the tutorial presented here: http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Plants Database\n",
    "\n",
    "This is perhaps the best known database to be found in the pattern recognition literature.  \n",
    "The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  \n",
    "One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.  \n",
    "\n",
    "We have measurements from 150 Iris flowers from 3 different species.\n",
    "\n",
    "Attribute Information:\n",
    "   1. sepal length in cm\n",
    "   2. sepal width in cm\n",
    "   3. petal length in cm\n",
    "   4. petal width in cm\n",
    "   5. class:  \n",
    "      -- Iris Setosa  \n",
    "      -- Iris Versicolour  \n",
    "      -- Iris Virginica  \n",
    "\n",
    "Sources:  \n",
    "     (a) Creator: R.A. Fisher  \n",
    "     (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)  \n",
    "     (c) Date: July, 1988  \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"images/iris_with_labels.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"images/iris_types.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets import the dataset and play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "df = pd.read_csv('ds/iris.data', header=None, names=columns)\n",
    "df.dropna(how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('class').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we seperate the dataframe into class labels and an attribute matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = df.iloc[:, :4]\n",
    "classes = df.iloc[:, 4].values\n",
    "mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does each feature vary across species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {1: 'Iris-Setosa',\n",
    "              2: 'Iris-Versicolor',\n",
    "              3: 'Iris-Virgnica'}\n",
    "\n",
    "feature_dict = {0: 'sepal_length',\n",
    "                1: 'sepal_width',\n",
    "                2: 'petal_length',\n",
    "                3: 'petal_width'}\n",
    "\n",
    "with plt.style.context('seaborn-darkgrid'):\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    for cnt in range(4):\n",
    "        plt.subplot(2, 2, cnt+1)\n",
    "        for lab in ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'):\n",
    "            plt.hist(df[df['class'] == lab][feature_dict[cnt]],\n",
    "                     label=lab,\n",
    "                     bins=10,\n",
    "                     alpha=0.9,)\n",
    "        plt.xlabel(feature_dict[cnt])\n",
    "    \n",
    "    plt.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Summary of the PCA Approach\n",
    "* Standardize the data.\n",
    "* Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n",
    "* Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues where $k$ is the number of dimensions of the new feature subspace ($k \\leq d$).\n",
    "* Construct the projection matrix $W$ from the selected $k$ eigenvectors.\n",
    "* Transform the original dataset $X$ via $W$ to obtain a $k$-dimensional feature subspace $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Preprocessing/Scaling\n",
    " We'll scale the data to have a mean of 0 and variance of 1  \n",
    " We can use the StandardScaler class from sklearn for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "mat_std = scaler.fit_transform(mat)\n",
    "print(mat.values[0])\n",
    "print(mat_std[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Eigendecomposition\n",
    "\n",
    "The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the “core” of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.\n",
    "\n",
    "The classic approach to PCA is to perform the eigendecomposition on the covariance matrix $\\Sigma$, which is a \n",
    "$d \\times d$ matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:\n",
    "\n",
    "$$\\sigma_{jk} = \\frac{1}{n-1} (X - \\overline{x})^T (X - \\overline{x})$$\n",
    "\n",
    "where $\\overline{x}$ is the mean vector, $\\frac{1}{n} \\sum_{i=i}^{n} x_i$\n",
    "\n",
    "The mean vector is a $d$-dimensional vector where each value in this vector represents the sample mean of a feature column in the dataset.\n",
    "\n",
    "### We'll use the covariance matrix approach, but you could also use the correlation matrix or perform Singular Vector Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec = np.mean(mat_std, axis=0) # by column\n",
    "cov_mat = (mat_std - mean_vec).T.dot((mat_std - mean_vec)) / (mat_std.shape[0] - 1)\n",
    "print(cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there a library for that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov_mat = np.cov(mat_std.T)\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eig_vecs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Selecting Principal Components\n",
    "\n",
    "In order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.\n",
    "In order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top $k$\n",
    "eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0],\": \",i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance\n",
    "After sorting the eigenpairs, the next question is “how many principal components are we going to choose for our new feature subspace?” A useful measure is the so-called “explained variance,” which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(eig_vals)\n",
    "var_exp = [(i / total)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "cum_var_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n",
    "            label='individual explained variance')\n",
    "    plt.step(range(4), cum_var_exp, where='mid',\n",
    "             label='cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above clearly shows that most of the variance (72.77% of the variance to be precise) can be explained by the first principal component alone. The second principal component still bears some information (23.03%) while the third and fourth principal components can safely be dropped without losing to much information. Together, the first two principal components contain 95.8% of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Projection Matrix\n",
    "The construction of the projection matrix that will be used to transform the Iris data onto the new feature subspace. Although, the name “projection matrix” has a nice ring to it, it is basically just a matrix of our concatenated top k eigenvectors.\n",
    "\n",
    "\n",
    "Here, we are reducing the 4-dimensional feature space to a 2-dimensional feature subspace, by choosing the “top 2” eigenvectors with the highest eigenvalues to construct our $d \\times k$-dimensional eigenvector matrix $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),\n",
    "                      eig_pairs[1][1].reshape(4,1)))\n",
    "print(matrix_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Projecting onto the new feature space\n",
    "\n",
    "In this last step we will use the $4\\times2$-dimensional projection matrix $W$ to transform our samples onto the new subspace via the equation\n",
    "$$Y=X\\times W$$\n",
    ", where $Y$ is a $150\\times 2$ matrix of our transformed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = mat_std.dot(matrix_w)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we have after applying the linear PCA transformation is a lower dimensional subspace (from 4D to 2D in this case), where the samples are “most spread” along the new feature axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n",
    "                        ('blue', 'red', 'green')):\n",
    "        plt.scatter(Y[classes == lab, 0],\n",
    "                    Y[classes == lab, 1],\n",
    "                    label=lab,\n",
    "                    c=col)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "Y_sklearn = sklearn_pca.fit_transform(mat_std)\n",
    "\n",
    "# 0,1 denote PC1 and PC2; change values for other PCs\n",
    "xvector = sklearn_pca.components_[0] \n",
    "yvector = sklearn_pca.components_[1]\n",
    "\n",
    "xs = sklearn_pca.transform(mat_std)[:,0] \n",
    "ys = sklearn_pca.transform(mat_std)[:,1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n",
    "                        ('blue', 'red', 'green')):\n",
    "        plt.scatter(Y_sklearn[classes==lab, 0],\n",
    "                    Y_sklearn[classes==lab, 1],\n",
    "                    label=lab,\n",
    "                    c=col)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    for i in range(len(xvector)):\n",
    "# arrows project features (ie columns from csv) as vectors onto PC axes\n",
    "        plt.arrow(0, 0, xvector[i]*max(xs), yvector[i]*max(ys),\n",
    "              color='r', width=0.0005, head_width=0.0025)\n",
    "    \n",
    "        plt.text(xvector[i]*max(xs)*1.2, yvector[i]*max(ys)*1.2,\n",
    "             list(df.columns.values)[i], color='black')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xvector)\n",
    "print(yvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
